{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning to find the optimal dictionary $D$ s.t. $\\min_x ||Dx||_\\epsilon + \\frac{1}{2}||x-f||^2$ is a good denoising of data $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as ply\n",
    "import plotly.graph_objs as go\n",
    "ply.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "from IPython.display import clear_output\n",
    "%load_ext autoreload\n",
    "\n",
    "from PIL import Image\n",
    "import bilevelsurrogates as Sur\n",
    "\n",
    "#ymmv:\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sur.deterministic() # Disable this for reasonable timings, enable to reproduce numbers from paper\n",
    "# Note that the actual timing depends on the number of iterations of the subproblems, which can vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data first!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/vsa_jonas/Dropbox/Documents_Hyperion/Python/ParametricMajorizationICCVVersion/data/BSDS300/iids_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8edf2e11c3da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data/BSDS300/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m datasetTrain = Sur.data.BSDS300_for_denoising(path, noise_std=noise, augmentations=patch_cropping, flip=True,\n\u001b[0;32m---> 10\u001b[0;31m                                               grayscale=True, clip_to_realistic=clip)\n\u001b[0m\u001b[1;32m     11\u001b[0m datasetTest = Sur.data.BSDS300_for_denoising(path, split = 'trainingZhang', noise_std=noise,\n\u001b[1;32m     12\u001b[0m                                              \u001b[0maugmentations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_cropping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents_Hyperion/Python/ParametricMajorizationICCVVersion/ParametricMajorization/bilevelsurrogates/data/denoising_data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pathdir, split, img_size, grayscale, clip_to_realistic, augmentations, normalize, noise_std, resize, flip)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mdict_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iids_train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/vsa_jonas/Dropbox/Documents_Hyperion/Python/ParametricMajorizationICCVVersion/data/BSDS300/iids_train.txt'"
     ]
    }
   ],
   "source": [
    "noise = 25/255\n",
    "clip = False\n",
    "batch_size = 200\n",
    "\n",
    "patch_cropping = torchvision.transforms.RandomCrop((64,64))\n",
    "#patch_cropping = None\n",
    "\n",
    "path = os.getcwd() + '/data/BSDS300/' \n",
    "datasetTrain = Sur.data.BSDS300_for_denoising(path, noise_std=noise, augmentations=patch_cropping, flip=True,\n",
    "                                              grayscale=True, clip_to_realistic=clip)\n",
    "datasetTest = Sur.data.BSDS300_for_denoising(path, split = 'trainingZhang', noise_std=noise,\n",
    "                                             augmentations=patch_cropping,\n",
    "                                             flip=True, grayscale=True, clip_to_realistic=clip)\n",
    "datasetTestFull = Sur.data.BSDS300_for_denoising(path, split = 'testing68Zhang', noise_std=noise,\n",
    "                                             augmentations=None, flip=True, grayscale=True,\n",
    "                                             clip_to_realistic=clip)\n",
    "datasetTest12 = Sur.data.Test12_for_denoising(os.path.dirname(os.getcwd()) + '/data/test12/', noise_std=noise,\n",
    "                                             augmentations=None, clip_to_realistic=clip)\n",
    "samples = Sur.data.Samples(datasetTrain, batch_size, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example training patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,4,figsize=(20, 10))\n",
    "for idx, img_id in enumerate(np.random.choice(batch_size,4)):\n",
    "    axes[0,idx].imshow(datasetTrain.unnormalize(samples.x[img_id,0,:,:]).cpu(), cmap='gray')\n",
    "    axes[1,idx].imshow(datasetTrain.unnormalize(samples.y[img_id,0,:,:]).cpu(), cmap='gray')\n",
    "fig.canvas.draw()\n",
    "print(f'noisy psnr: {Sur.psnr_compute(samples.y, samples.x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,4,figsize=(20, 10))\n",
    "for idx, img_id in enumerate(np.random.choice(len(datasetTest),4)):\n",
    "    test_gt, test_noisy = datasetTest[img_id]\n",
    "    axes[0,idx].imshow(datasetTrain.unnormalize(test_gt[0,:,:]).cpu(), cmap='gray')\n",
    "    axes[1,idx].imshow(datasetTrain.unnormalize(test_noisy[0,:,:]).cpu(), cmap='gray')\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Energy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_setup = dict()\n",
    "model_setup['data'] = dict(dtype=dtype, device=device)\n",
    "\n",
    "# Sizes:\n",
    "model_setup['x_dims'] = samples.x.shape\n",
    "\n",
    "# Hyperparameters:\n",
    "model_setup['alpha'] = 0.01\n",
    "model_setup['epsilon'] = 0.0\n",
    "model_setup['bias'] = False\n",
    "model_setup['norm'] = 'aniso'\n",
    "model_setup['clip'] = True\n",
    "\n",
    "model_setup['inference'] = dict(tolerance=1e-4, max_iterations=250)\n",
    "\n",
    "\n",
    "# Setups 1, 2, 3 correspond to table 1 in the paper:\n",
    "setup = 2\n",
    "\n",
    "# Define Model\n",
    "def energy_setup(model_setup):\n",
    "    if setup == 1:\n",
    "        dictionary = Sur.DCTConvolution(in_channels=1, out_channels=3, kernel_size=3, mean=False,\n",
    "                                        bias=model_setup['bias']).to(**model_setup['data'])\n",
    "    elif setup == 2:\n",
    "        dictionary = Sur.DCTConvolution(in_channels=1, out_channels=48, kernel_size=7, mean=False,\n",
    "                                        bias=model_setup['bias']).to(**model_setup['data'])\n",
    "    elif setup == 3:\n",
    "        dictionary = Sur.DCTConvolution(in_channels=1, out_channels=96, kernel_size=9, mean=False,\n",
    "                                        bias=model_setup['bias']).to(**model_setup['data'])\n",
    "        model_setup['alpha'] *= (48*7*7) / (96*9*9)\n",
    "    elif setup == 4:\n",
    "        dictionary = Sur.Gradient(in_channels=1, scalable=True).to(**model_setup['data'])\n",
    "    elif setup == 5:\n",
    "        dictionary = Sur.DCTConvolution(in_channels=1, out_channels=168, kernel_size=13, mean=False,\n",
    "                                        bias=model_setup['bias']).to(**model_setup['data'])\n",
    "        model_setup['alpha'] *= (48*7*7) / (168*13**2)    \n",
    "    energy = Sur.model.AnalysisSparsity(dictionary, setup=model_setup)\n",
    "    return energy\n",
    "energy = energy_setup(model_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Higher-level loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Sur.loss.PSNR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Training Algorithms ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize initial dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sur.visualize(energy.operator);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.backends.cudnn.deterministic = False # enable this for non-deterministic timings with det. data selection and init\n",
    "# torch.backends.cudnn.benchmark = True # enable this for non-deterministic timings with det. data selection and init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Single step Parametric Majorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'joint-dual'\n",
    "training_setup = Sur.training.default_setup('DiscriminativeLearning', algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = energy_setup(model_setup)\n",
    "optimizer = Sur.training.DiscriminativeLearning(energy, loss, samples, training_setup, algorithm=algorithm)\n",
    "optimizer.run();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sur.visualize(energy.operator)\n",
    "img_denoised, _ = Sur.training_error(energy, loss, samples);\n",
    "samples.redraw_noise()\n",
    "img_denoised, _ = Sur.training_error(energy, loss, samples);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = Sur.testing_error(energy, loss, datasetTest, batch_size=25);\n",
    "_, _ = Sur.testing_error(energy, loss, datasetTestFull);\n",
    "_, _ = Sur.testing_error(energy, loss, datasetTest12, batch_size=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3,figsize=(20, 10))\n",
    "axes[0].imshow(samples.x[0,:,:,].permute(1, 2, 0).cpu().numpy().squeeze(), cmap='gray')\n",
    "axes[1].imshow(samples.y[0,:,:,].permute(1, 2, 0).cpu().numpy().squeeze(), cmap='gray')\n",
    "axes[2].imshow(img_denoised[0,:,:,:].permute(1, 2, 0).cpu().numpy().squeeze(), cmap='gray')\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = go.Layout(\n",
    "         xaxis=dict(\n",
    "             type='log',\n",
    "             autorange=True,\n",
    "             title='Iterations',\n",
    "             automargin=True\n",
    "             ),\n",
    "         yaxis=dict(\n",
    "             type='log',\n",
    "             autorange  = True,\n",
    "             title='Value',\n",
    "             automargin=True\n",
    "         ),\n",
    "         )\n",
    "\n",
    "data = [go.Scatter(y=optimizer.stats['loss'], name = 'Discriminator loss')]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "ply.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Iterated Parametric Majorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_setup = Sur.training.default_setup('IterativeLearning', algorithm)\n",
    "iterative_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "energy = energy_setup(model_setup)\n",
    "subroutine = Sur.training.DiscriminativeLearning(energy, loss, samples, training_setup, algorithm=algorithm)\n",
    "optimizer =  Sur.training.IterativeLearning(subroutine, iterative_setup)\n",
    "optimizer.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sur.visualize(energy.operator);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_denoised, _ = Sur.training_error(energy, loss, samples);\n",
    "samples.redraw_noise()\n",
    "img_denoised, _ = Sur.training_error(energy, loss, samples);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.reset()\n",
    "samples.step()\n",
    "img_denoised, _ = Sur.training_error(energy, loss, samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model_setup, training_setup, iterative_setup, energy.state_dict()\n",
    "path = './energies/'+algorithm+str(datetime.date.today()) + '.pt'\n",
    "#torch.save(data, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = Sur.testing_error(energy, loss, datasetTest, batch_size=25);\n",
    "_, _ = Sur.testing_error(energy, loss, datasetTestFull, batch_size=4);\n",
    "_, _ = Sur.testing_error(energy, loss, datasetTest12, batch_size=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3,figsize=(20, 10))\n",
    "axes[0].imshow(samples.x[0,:,:,].permute(1, 2, 0).cpu().numpy().squeeze(), cmap='gray')\n",
    "axes[1].imshow(samples.y[0,:,:,].permute(1, 2, 0).cpu().numpy().squeeze(), cmap='gray')\n",
    "axes[2].imshow(img_denoised[0,:,:,:].permute(1, 2, 0).cpu().numpy().squeeze(), cmap='gray')\n",
    "fig.canvas.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
