"""
    Abstract class to collect information about the 'higher-level' loss function, used to train an energy-based model
    containing the evaluation of the loss function, its gradients w.r.t. to first and second argument and evaluations
    of the actual metric that is targeted.
"""


class Loss:
    """
        Abstract class, containing necessary methods
    """
    def __init__(self):
        pass

    def __call__(self, reference, argmin):
        """
            Return l(x^*,x(theta))
        """
        raise NotImplementedError()
        return value, name, format

    def gradient(self, reference, argmin):
        """
            Return the gradient of l(x^*,x(theta)) w.r.t to the second argument
        """
        raise NotImplementedError()
        return vector

    def metric(self, reference, argmin):
        """
            The actually sought metric
        """
        raise NotImplementedError()
        return value, name, format

    def metric_est(self, majorizer_loss):
        """
            Estimate from upper bound
        """
        raise NotImplementedError()
        return value, name, format


class BregmanLoss(Loss):
    """
        A loss function that can be written as Bregman distance D_w(x^*,x(theta))
        generated by some Legendre function w:R^n to R
    """
    def legendre(self, x):
        """
            Returns the generating Legendre function w(x)
        """
        raise NotImplementedError()
        return value

    def legendre_gradient(self, x):
        """
            Returns the gradient of the generating Legendre function w(x)
        """
        raise NotImplementedError()
        return vector

    def __call__(self, reference, argmin):
        """
            Return l(x^*,x(theta))
        """
        linearization = (self.legendre_gradient(argmin) * (reference - argmin)).sum()
        return self.legendre(reference) - self.legendre(argmin) - lineraization

    def gradient_first(self, arg1, arg2):
        """
            Return the gradient of l(x^*,x(theta)) w.r.t to the first argument
        """
        return self.legendre_gradient(arg1) - self.legendre_gradient(arg2)
